{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Loss functions.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aditya0811/-100daysofMLcode/blob/master/Loss_functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "XgMli-V873_0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "[**Types of Loss Functions**](https://isaacchanghau.github.io/post/loss_functions/)\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "bR6OL-UW0oK5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Mean Squared Error**\n",
        "\n",
        "\n",
        "\n",
        "*   minimises sum of suared distances of the points for the line \n",
        "*   using Sigmoid activation functions is not preferred here.\n",
        "For example, by using Sigmoid, \n",
        "^\n",
        "y\n",
        "(\n",
        "i\n",
        ")\n",
        "=\n",
        "σ\n",
        "(\n",
        "z\n",
        "(\n",
        "i\n",
        ")\n",
        ")\n",
        "=\n",
        "σ\n",
        "(\n",
        "θ\n",
        "T\n",
        "x\n",
        "(\n",
        "i\n",
        ")\n",
        ")\n",
        ", simply, we only consider one sample, say, \n",
        "(\n",
        "y\n",
        "−\n",
        "σ\n",
        "(\n",
        "z\n",
        ")\n",
        ")\n",
        "2\n",
        ", and it derivative is computed by\n",
        "∂\n",
        "L\n",
        "∂\n",
        "θ\n",
        "=\n",
        "−\n",
        "(\n",
        "y\n",
        "−\n",
        "σ\n",
        "(\n",
        "z\n",
        ")\n",
        ")\n",
        "⋅\n",
        "σ\n",
        "′\n",
        "(\n",
        "z\n",
        ")\n",
        "⋅\n",
        "x\n",
        "according to the shape and feature of Sigmoid, when \n",
        "σ\n",
        "(\n",
        "z\n",
        ")\n",
        " tends to 0 or 1, \n",
        "σ\n",
        "′\n",
        "(\n",
        "z\n",
        ")\n",
        " is close to zero, and when \n",
        "σ\n",
        "(\n",
        "z\n",
        ")\n",
        " close to 0.5, \n",
        "σ\n",
        "′\n",
        "(\n",
        "z\n",
        ")\n",
        " will reach it maximum. In this case, when the difference between predicted value and true label \n",
        "(\n",
        "y\n",
        "−\n",
        "σ\n",
        "(\n",
        "z\n",
        ")\n",
        ")\n",
        " is large, \n",
        "σ\n",
        "′\n",
        "(\n",
        "z\n",
        ")\n",
        " will close to 0, which decreases the convergence speed, this is improper, since we expect that the learning speed should be fast when the error is large.\n",
        "\n",
        "\n",
        "\n",
        "*   MSE is more useful if concerning about large errors whose consequences are much bigger than equivalent smaller ones\n",
        "*   MSE also corresponds to maximizing the likelihood of Gaussian random variables.\n",
        "* Gradient easier to compute than MSE \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "sAqSyUwe2VII",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Mean Squared Logarithmic Error**\n",
        "\n",
        "\n",
        "*   Used when we want to ignore large differences between prediction and true value if both of them are large.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "iORkaWhb3Gf_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**L2**\n",
        "\n",
        "L2 loss function is the square of the L2 norm of the difference between actual value and predicted value. It is mathematically similar to MSE, only do not have division by \n",
        "n "
      ]
    },
    {
      "metadata": {
        "id": "cdz5HGuf34St",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Mean Absolute Error**\n",
        "\n",
        "\n",
        "*   MAE is more robust to outliers since it does not make use of square. \n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "dOUPXJ3D4ddV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**L1**\n",
        "\n",
        "\n",
        "*   L1 is mathematically similar to MAE, only do not have division by \n",
        "n\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "KGlrqOE15RUZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Cross Entropy**\n",
        "\n",
        "\n",
        "\n",
        "*   Used for binary Classification\n",
        "![for binary class](https://gombru.github.io/assets/cross_entropy_loss/intro.png)\n",
        "* Here we can use Sigmoid activation as after differentiating we will only have for deciding convergence \n",
        "y\n",
        "−\n",
        "σ\n",
        "(\n",
        "z\n",
        ")\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "moGMBWrv6p1Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Negative Logarithmic Likelihood**\n",
        "*   Used to find probability of the given class,mostly used in Neural Network"
      ]
    },
    {
      "metadata": {
        "id": "k5JWCsI94c5S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jzENURS71apt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}